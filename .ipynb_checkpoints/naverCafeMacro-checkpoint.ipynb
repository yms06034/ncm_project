{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66ecd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\user\\anaconda3\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (0.10.3)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (2023.5.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.1)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: BeautifulSoup4 in c:\\users\\user\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from BeautifulSoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: webdriver_manager in c:\\users\\user\\anaconda3\\lib\\site-packages (3.8.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from webdriver_manager) (4.64.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\user\\anaconda3\\lib\\site-packages (from webdriver_manager) (1.0.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from webdriver_manager) (2.28.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from webdriver_manager) (22.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2023.5.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm->webdriver_manager) (0.4.6)\n",
      "Requirement already satisfied: pyperclip in c:\\users\\user\\anaconda3\\lib\\site-packages (1.8.2)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\user\\anaconda3\\lib\\site-packages (1.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install BeautifulSoup4\n",
    "!pip install requests\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install webdriver_manager\n",
    "!pip install pyperclip\n",
    "!pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8c0dfe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "99bc251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def css_finds(css_selector):\n",
    "    return browser.find_elements(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "def css_find(css_selector):\n",
    "    return browser.find_element(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "def find(wait, css_selector):\n",
    "    return wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, css_selector)))\n",
    "\n",
    "def finds_xpath(xpath):\n",
    "    return browser.find_elements(By.XPATH, xpath)\n",
    "\n",
    "def find_xpath(xpath):\n",
    "    return browser.find_element(By.XPATH, xpath)\n",
    "\n",
    "def find_id(id_x):\n",
    "    return browser.find_element(By.ID, id_x)\n",
    "\n",
    "# Options Setting\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('no-sandox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--start-maximized')\n",
    "options.add_argument('incognito')\n",
    "# options.add_argument('headless')\n",
    "# Header Setting\n",
    "service = Service(ChromeDriverManager().install())\n",
    "browser = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "\n",
    "# Settings\n",
    "NAVER_ID = \"sjk5838\"\n",
    "NAVER_PW = \"rlatjdwls00@K\"\n",
    "\n",
    "CAFENAME = \"1motion1\"\n",
    "BORADTITLE = \"전체글보기\"\n",
    "\n",
    "keyword = \"소파\"\n",
    "COMMENTS = \"테스트 댓글 입니다.\"\n",
    "\n",
    "# def 1\n",
    "# Crawling Start\n",
    "browser.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "browser.implicitly_wait(2)\n",
    "\n",
    "input_id = find_id('id')\n",
    "input_pw = find_id('pw')\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "pyperclip.copy(NAVER_ID) \n",
    "input_id.send_keys(Keys.CONTROL, \"v\")\n",
    "\n",
    "pyperclip.copy(NAVER_PW) \n",
    "input_pw.send_keys(Keys.CONTROL, \"v\")\n",
    "input_pw.send_keys(\"\\n\")\n",
    "\n",
    "# Not needed when it's headless\n",
    "# no_save_btn = find_id('new.dontsave')\n",
    "# no_save_btn.click()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# def 2\n",
    "browser.get(f\"https://cafe.naver.com/{CAFENAME}\")\n",
    "time.sleep(2)\n",
    "\n",
    "boardName = browser.find_element(By.LINK_TEXT, f'{BORADTITLE}')\n",
    "\n",
    "boardName.click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "browser.switch_to.frame(\"cafe_main\")\n",
    "\n",
    "browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/a').click()\n",
    "browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/ul/li[7]/a').click()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "soup = BS(browser.page_source, \"html.parser\")\n",
    "soup = soup.find_all(class_='article-board m-tcol-c')[1]\n",
    "\n",
    "datas = soup.find_all(class_='td_article')\n",
    "dates = soup.find_all(class_='td_date')\n",
    "\n",
    "a_hrefs = soup.find_all(\"a\")\n",
    "\n",
    "# def 3\n",
    "post_hrefs = []\n",
    "for href in a_hrefs:\n",
    "    if keyword in href.text:\n",
    "        post_hrefs.append(href[\"href\"])\n",
    "\n",
    "final_hrefs = []\n",
    "\n",
    "for href in post_hrefs:\n",
    "    parsed_url = urlparse(href)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    article_id = query_params['articleid'][0]\n",
    "    club_id = query_params['clubid'][0]\n",
    "    new_url = f\"https://cafe.naver.com/{CAFENAME}?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D{club_id}%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
    "    \n",
    "    final_hrefs.append(new_url)\n",
    "\n",
    "# def 4\n",
    "for r_href in final_hrefs:\n",
    "    browser.get(r_href)\n",
    "    browser.switch_to.frame(\"cafe_main\")\n",
    "    \n",
    "    text_area = find_xpath('//*[@id=\"app\"]/div/div/div[2]/div[2]/div[5]/div[2]/div[1]/textarea')\n",
    "    text_area.click()\n",
    "\n",
    "pyperclip.copy(COMMENTS) \n",
    "text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "browser.find_element(By.XPATH, '/html/body/div/div/div/div[2]/div[2]/div[5]/div[2]/div[2]/div[2]/a').click()\n",
    "\n",
    "browser.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "67e4d5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "final_hrefs = []\n",
    "\n",
    "for href in post_hrefs:\n",
    "    parsed_url = urlparse(href)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    article_id = query_params['articleid'][0]\n",
    "    club_id = query_params['clubid'][0]\n",
    "    new_url = f\"https://cafe.naver.com/1motion1?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D{club_id}%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
    "    \n",
    "    final_hrefs.append(new_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "28f2f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "for po in post_hrefs:\n",
    "    da_1 = po.split(\"&\")\n",
    "    print(da_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d1393",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = browser.find_elements(By.CSS_SELECTOR, '.comment')\n",
    "for comment in comments:\n",
    "    nickname = comment.find_element_by_css_selector('.user_name').text\n",
    "    \n",
    "    if nickname == my_nickname:\n",
    "        # 다른 URL로 넘어가는 로직 작성\n",
    "        continue\n",
    "    \n",
    "    comment_textarea = comment.find_element_by_css_selector('.comment_input textarea')\n",
    "    comment_textarea.send_keys(comment_content)\n",
    "    comment_button = comment.find_element_by_css_selector('.comment_input .comment_submit')\n",
    "    comment_button.click()\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0a3d6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "crawling_page = int(math.ceil(100 / 50)+1)\n",
    "print(crawling_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "da3d2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def css_finds(css_selector):\n",
    "    return browser.find_elements(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "def css_find(css_selector):\n",
    "    return browser.find_element(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "def find(wait, css_selector):\n",
    "    return wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, css_selector)))\n",
    "\n",
    "def finds_xpath(xpath):\n",
    "    return browser.find_elements(By.XPATH, xpath)\n",
    "\n",
    "def find_xpath(xpath):\n",
    "    return browser.find_element(By.XPATH, xpath)\n",
    "\n",
    "def find_id(id_x):\n",
    "    return browser.find_element(By.ID, id_x)\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "browser = webdriver.Chrome(service=service, options=options)    \n",
    "\n",
    "# Settings\n",
    "NAVER_ID = \"sjk5838\"\n",
    "NAVER_PW = \"rlatjdwls00@K\"\n",
    "\n",
    "CAFENAME = \"1motion1\"\n",
    "BORADTITLE = \"전체글보기\"\n",
    "\n",
    "keyword = \"소소소소파\"\n",
    "COMMENTS = \"테스트 댓글 입니다.\"\n",
    "\n",
    "# def 1\n",
    "# Crawling Start\n",
    "\n",
    "while True:\n",
    "    browser.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "    browser.implicitly_wait(2)\n",
    "\n",
    "    input_id = find_id('id')\n",
    "    input_pw = find_id('pw')\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    pyperclip.copy(NAVER_ID)\n",
    "    input_id.send_keys(Keys.CONTROL, \"v\")\n",
    "\n",
    "    pyperclip.copy(NAVER_PW) \n",
    "    input_pw.send_keys(Keys.CONTROL, \"v\")\n",
    "    input_pw.send_keys(\"\\n\")\n",
    "\n",
    "    # Not needed when it's headless\n",
    "    # no_save_btn = find_id('new.dontsave')\n",
    "    # no_save_btn.click()\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    # def 2\n",
    "    browser.get(f\"https://cafe.naver.com/{CAFENAME}\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    boardName = browser.find_element(By.LINK_TEXT, f'{BORADTITLE}')\n",
    "\n",
    "    boardName.click()\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    browser.switch_to.frame(\"cafe_main\")\n",
    "\n",
    "    browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/a').click()\n",
    "    browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/ul/li[7]/a').click()\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    soup = BS(browser.page_source, \"html.parser\")\n",
    "    soup = soup.find_all(class_='article-board m-tcol-c')[1]\n",
    "\n",
    "    datas = soup.find_all(class_='td_article')\n",
    "    dates = soup.find_all(class_='td_date')\n",
    "\n",
    "    a_hrefs = soup.find_all(\"a\")\n",
    "\n",
    "    # def 3\n",
    "    post_hrefs = []\n",
    "    for href in a_hrefs:\n",
    "        if keyword in href.text:\n",
    "            post_hrefs.append(href[\"href\"])\n",
    "\n",
    "    final_hrefs = []\n",
    "\n",
    "    for href in post_hrefs:\n",
    "        parsed_url = urlparse(href)\n",
    "        query_params = parse_qs(parsed_url.query)\n",
    "        article_id = query_params['articleid'][0]\n",
    "        club_id = query_params['clubid'][0]\n",
    "        new_url = f\"https://cafe.naver.com/{CAFENAME}?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D{club_id}%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
    "\n",
    "        final_hrefs.append(new_url)\n",
    "\n",
    "# def 4\n",
    "\n",
    "# browser.get(\"https://cafe.naver.com/1motion1?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D28520242%2526page%3D1%2526boardtype%3DL%2526articleid%3D553%2526referrerAllArticles%3Dtrue\")\n",
    "\n",
    "# time.sleep(1)\n",
    "\n",
    "# browser.switch_to.frame(\"cafe_main\")\n",
    "\n",
    "# time.sleep(1)\n",
    "# comments_not_allowed = browser.find_element(By.CSS_SELECTOR, 'button_comment not_allowed')\n",
    "# print(comments_not_allowed)\n",
    "\n",
    "# nickname = NICKNAME\n",
    "# nicksname = browser.find_element(By.CLASS_NAME, 'comment_inbox_name').text\n",
    "\n",
    "# cmtNicks = browser.find_elements(By.CLASS_NAME, 'comment_nickname')\n",
    "# text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "# text_area.click()\n",
    "\n",
    "# pyperclip.copy(COMMENTS)\n",
    "# text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "\n",
    "# time.sleep(2)\n",
    "\n",
    "# register_btn = browser.find_element(By.CLASS_NAME, 'btn_register')\n",
    "# register_btn.click()\n",
    "\n",
    "\n",
    "cmtnicks = []\n",
    "\n",
    "for p_href in final_hrefs:\n",
    "    browser.get(p_href)\n",
    "    time.sleep(1)\n",
    "    browser.switch_to.frame(\"cafe_main\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    try:\n",
    "        nicksname = browser.find_element(By.CLASS_NAME, 'comment_inbox_name').text\n",
    "        cmtNicks = browser.find_elements(By.CLASS_NAME, 'comment_nickname')\n",
    "\n",
    "        if cmtNicks:\n",
    "            for cmtNick in cmtNicks:\n",
    "                cmtnick = cmtNick.text\n",
    "                cmtnicks.append(cmtnick)\n",
    "\n",
    "            if nickname in cmtnicks:\n",
    "                # 내 닉네임으로 댓글이 있는 경우, 다음 페이지로 넘어감\n",
    "                continue\n",
    "            else:\n",
    "                # 내 닉네임으로 댓글이 없는 경우, 댓글 작성\n",
    "                time.sleep(1)\n",
    "                text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "                text_area.click()\n",
    "\n",
    "                pyperclip.copy(COMMENTS) \n",
    "                text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "                register_btn = browser.find_element(By.CLASS_NAME, 'btn_register')\n",
    "                register_btn.click()\n",
    "        else:\n",
    "            # 댓글 작성\n",
    "            time.sleep(1)\n",
    "            text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "            text_area.click()\n",
    "\n",
    "            pyperclip.copy(COMMENTS) \n",
    "            text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "            register_btn = browser.find_element(By.CLASS_NAME, 'btn_register')\n",
    "            register_btn.click()\n",
    "            \n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "    cmtnicks.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fdb814f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51f9fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmtnicks = []\n",
    "\n",
    "for p_href in final_hrefs:\n",
    "    browser.get(p_href)\n",
    "    time.sleep(1)\n",
    "    browser.switch_to.frame(\"cafe_main\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    nickname = NICKNAME\n",
    "\n",
    "    cmtNicks = browser.find_elements(By.CLASS_NAME, 'comment_nickname')\n",
    "    \n",
    "    if cmtNicks:\n",
    "        for cmtNick in cmtNicks:\n",
    "            cmtnick = cmtNick.text\n",
    "            cmtnicks.append(cmtnick)\n",
    "\n",
    "        if nickname in cmtnicks:\n",
    "            # 내 닉네임으로 댓글이 있는 경우, 다음 페이지로 넘어감\n",
    "            continue\n",
    "        else:\n",
    "            # 내 닉네임으로 댓글이 없는 경우, 댓글 작성\n",
    "            time.sleep(1)\n",
    "            text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "            text_area.click()\n",
    "\n",
    "            pyperclip.copy(COMMENTS) \n",
    "            text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "            browser.find_element(By.XPATH, '/html/body/div/div/div/div[2]/div[2]/div[5]/div[2]/div[2]/div[2]/a').click()\n",
    "    else:\n",
    "        # 댓글 작성\n",
    "        time.sleep(1)\n",
    "        text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "        text_area.click()\n",
    "\n",
    "        pyperclip.copy(COMMENTS) \n",
    "        text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "        browser.find_element(By.XPATH, '/html/body/div/div/div/div[2]/div[2]/div[5]/div[2]/div[2]/div[2]/a').click()\n",
    "\n",
    "    cmtnicks.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0241bb7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (236959528.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[61], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    def crawling_start(NAVER_ID, NAVER_PW, CAFENAME, keyword = \"전체글보기\", COMMENTS):\u001b[0m\n\u001b[1;37m                                                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def crawling_start(NAVER_ID, NAVER_PW, CAFENAME, keyword = \"전체글보기\", COMMENTS):\n",
    "    def selenuim_setting():\n",
    "        # Options Setting\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('no-sandox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--start-maximized')\n",
    "        options.add_argument('incognito')\n",
    "        # options.add_argument('headless')\n",
    "        # Header Setting\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        browser = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        return browser\n",
    "\n",
    "    def naverMarco(browser):\n",
    "        def css_finds(css_selector):\n",
    "            return browser.find_elements(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "        def css_find(css_selector):\n",
    "            return browser.find_element(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "        def find(wait, css_selector):\n",
    "            return wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, css_selector)))\n",
    "\n",
    "        def finds_xpath(xpath):\n",
    "            return browser.find_elements(By.XPATH, xpath)\n",
    "\n",
    "        def find_xpath(xpath):\n",
    "            return browser.find_element(By.XPATH, xpath)\n",
    "\n",
    "        def find_id(id_x):\n",
    "            return browser.find_element(By.ID, id_x)\n",
    "\n",
    "        \n",
    "        browser.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "        browser.implicitly_wait(2)\n",
    "\n",
    "        input_id = find_id('id')\n",
    "        input_pw = find_id('pw')\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        pyperclip.copy(NAVER_ID) \n",
    "        input_id.send_keys(Keys.CONTROL, \"v\")\n",
    "\n",
    "        pyperclip.copy(NAVER_PW) \n",
    "        input_pw.send_keys(Keys.CONTROL, \"v\")\n",
    "        input_pw.send_keys(\"\\n\")\n",
    "\n",
    "        time.sleep(1)\n",
    "        \n",
    "        browser.get(f\"https://cafe.naver.com/{CAFENAME}\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "\n",
    "        boardName = browser.find_element(By.LINK_TEXT, f'{BORADTITLE}')\n",
    "        boardName.click()\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        browser.switch_to.frame(\"cafe_main\")\n",
    "\n",
    "        browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/a').click()\n",
    "        browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/ul/li[7]/a').click()\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        soup = BS(browser.page_source, \"html.parser\")\n",
    "        soup = soup.find_all(class_='article-board m-tcol-c')[1]\n",
    "\n",
    "        datas = soup.find_all(class_='td_article')\n",
    "        dates = soup.find_all(class_='td_date')\n",
    "\n",
    "        a_hrefs = soup.find_all(\"a\")\n",
    "        \n",
    "        \n",
    "        post_hrefs = []\n",
    "        \n",
    "        for href in a_hrefs:\n",
    "            if keyword in href.text:\n",
    "                post_hrefs.append(href[\"href\"])\n",
    "\n",
    "                \n",
    "        final_hrefs = []\n",
    "\n",
    "        for href in post_hrefs:\n",
    "            parsed_url = urlparse(href)\n",
    "            query_params = parse_qs(parsed_url.query)\n",
    "            article_id = query_params['articleid'][0]\n",
    "            club_id = query_params['clubid'][0]\n",
    "            new_url = f\"https://cafe.naver.com/{CAFENAME}?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D{club_id}%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
    "\n",
    "            final_hrefs.append(new_url)\n",
    "\n",
    "            \n",
    "        cmtnicks = []\n",
    "\n",
    "        for p_href in final_hrefs:\n",
    "            browser.get(p_href)\n",
    "            time.sleep(1)\n",
    "            browser.switch_to.frame(\"cafe_main\")\n",
    "            time.sleep(1)\n",
    "\n",
    "            try:\n",
    "                nicksname = browser.find_element(By.CLASS_NAME, 'comment_inbox_name').text\n",
    "                cmtNicks = browser.find_elements(By.CLASS_NAME, 'comment_nickname')\n",
    "\n",
    "                if cmtNicks:\n",
    "                    for cmtNick in cmtNicks:\n",
    "                        cmtnick = cmtNick.text\n",
    "                        cmtnicks.append(cmtnick)\n",
    "\n",
    "                    if nickname in cmtnicks:\n",
    "                        # 내 닉네임으로 댓글이 있는 경우, 다음 페이지로 넘어감\n",
    "                        continue\n",
    "                    else:\n",
    "                        # 내 닉네임으로 댓글이 없는 경우, 댓글 작성\n",
    "                        time.sleep(1)\n",
    "                        text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "                        text_area.click()\n",
    "\n",
    "                        pyperclip.copy(COMMENTS) \n",
    "                        text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "                        register_btn = browser.find_element(By.CLASS_NAME, 'btn_register')\n",
    "                        register_btn.click()\n",
    "                else:\n",
    "                    # 댓글 작성\n",
    "                    time.sleep(1)\n",
    "                    text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "                    text_area.click()\n",
    "\n",
    "                    pyperclip.copy(COMMENTS) \n",
    "                    text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "                    register_btn = browser.find_element(By.CLASS_NAME, 'btn_register')\n",
    "                    register_btn.click()\n",
    "\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "\n",
    "            cmtnicks.clear()\n",
    "            \n",
    "            df = pd.DataFrame(final_hrefs)\n",
    "            \n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f58bd8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cmtNicks:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ce49c6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cafe.naver.com/1motion1?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D28520242%2526page%3D1%2526boardtype%3DL%2526articleid%3D559%2526referrerAllArticles%3Dtrue\n"
     ]
    }
   ],
   "source": [
    "print(final_hrefs[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
