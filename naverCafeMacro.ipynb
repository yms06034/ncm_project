{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66ecd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (4.5.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (2.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: BeautifulSoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from BeautifulSoup4) (2.3.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (2.18.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (from requests) (2.6)\n",
      "Collecting urllib3<1.23,>=1.21.1\n",
      "  Using cached urllib3-1.22-py2.py3-none-any.whl (132 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.10.2 requires ruamel-yaml, which is not installed.\n",
      "selenium 4.5.0 requires urllib3[socks]~=1.26, but you have urllib3 1.22 which is incompatible.\n",
      "google-cloud-core 1.7.1 requires google-api-core<2.0.0dev,>=1.21.0, but you have google-api-core 2.11.0 which is incompatible.\n",
      "google-cloud-core 1.7.1 requires google-auth<2.0dev,>=1.24.0, but you have google-auth 2.16.0 which is incompatible.\n",
      "cookiecutter 1.7.3 requires requests>=2.23.0, but you have requests 2.18.4 which is incompatible.\n",
      "conda 22.9.0 requires requests>=2.20.1, but you have requests 2.18.4 which is incompatible.\n",
      "botocore 1.24.32 requires urllib3<1.27,>=1.25.4, but you have urllib3 1.22 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.14\n",
      "    Uninstalling urllib3-1.26.14:\n",
      "      Successfully uninstalled urllib3-1.26.14\n",
      "Successfully installed urllib3-1.22\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (1.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.21.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting webdriver_manager\n",
      "  Using cached webdriver_manager-3.8.6-py2.py3-none-any.whl (27 kB)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver_manager) (21.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver_manager) (4.64.0)\n",
      "Requirement already satisfied: requests in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (from webdriver_manager) (2.18.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->webdriver_manager) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (from requests->webdriver_manager) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2021.10.8)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (from requests->webdriver_manager) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (from requests->webdriver_manager) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->webdriver_manager) (0.4.4)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-1.0.0 webdriver-manager-3.8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script dotenv.exe is installed in 'C:\\Users\\yms06\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyperclip in c:\\users\\yms06\\appdata\\roaming\\python\\python39\\site-packages (1.8.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fake-useragent\n",
      "  Downloading fake_useragent-1.1.3-py3-none-any.whl (50 kB)\n",
      "Collecting importlib-resources>=5.0\n",
      "  Using cached importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-resources>=5.0->fake-useragent) (3.7.0)\n",
      "Installing collected packages: importlib-resources, fake-useragent\n",
      "Successfully installed fake-useragent-1.1.3 importlib-resources-5.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install BeautifulSoup4\n",
    "!pip install requests\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install webdriver_manager\n",
    "!pip install pyperclip\n",
    "!pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c0dfe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "99bc251a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[@id=\"app\"]/div/div/div[2]/div[2]/div[5]/div[2]/div[1]/textarea\"}\n  (Session info: chrome=114.0.5735.134); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\nBacktrace:\n\tGetHandleVerifier [0x0031A813+48355]\n\t(No symbol) [0x002AC4B1]\n\t(No symbol) [0x001B5358]\n\t(No symbol) [0x001E09A5]\n\t(No symbol) [0x001E0B3B]\n\t(No symbol) [0x0020E232]\n\t(No symbol) [0x001FA784]\n\t(No symbol) [0x0020C922]\n\t(No symbol) [0x001FA536]\n\t(No symbol) [0x001D82DC]\n\t(No symbol) [0x001D93DD]\n\tGetHandleVerifier [0x0057AABD+2539405]\n\tGetHandleVerifier [0x005BA78F+2800735]\n\tGetHandleVerifier [0x005B456C+2775612]\n\tGetHandleVerifier [0x003A51E0+616112]\n\t(No symbol) [0x002B5F8C]\n\t(No symbol) [0x002B2328]\n\t(No symbol) [0x002B240B]\n\t(No symbol) [0x002A4FF7]\n\tBaseThreadInitThunk [0x764C7D59+25]\n\tRtlInitializeExceptionChain [0x778DB74B+107]\n\tRtlClearBits [0x778DB6CF+191]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 112\u001b[0m\n\u001b[0;32m    109\u001b[0m     browser\u001b[38;5;241m.\u001b[39mget(r_href)\n\u001b[0;32m    110\u001b[0m     browser\u001b[38;5;241m.\u001b[39mswitch_to\u001b[38;5;241m.\u001b[39mframe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcafe_main\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 112\u001b[0m     text_area \u001b[38;5;241m=\u001b[39m \u001b[43mfind_xpath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m//*[@id=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m]/div/div/div[2]/div[2]/div[5]/div[2]/div[1]/textarea\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     text_area\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m    115\u001b[0m pyperclip\u001b[38;5;241m.\u001b[39mcopy(COMMENTS) \n",
      "Cell \u001b[1;32mIn[92], line 14\u001b[0m, in \u001b[0;36mfind_xpath\u001b[1;34m(xpath)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_xpath\u001b[39m(xpath):\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbrowser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:740\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    737\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    738\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 740\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:346\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    344\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:245\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    243\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[@id=\"app\"]/div/div/div[2]/div[2]/div[5]/div[2]/div[1]/textarea\"}\n  (Session info: chrome=114.0.5735.134); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\nBacktrace:\n\tGetHandleVerifier [0x0031A813+48355]\n\t(No symbol) [0x002AC4B1]\n\t(No symbol) [0x001B5358]\n\t(No symbol) [0x001E09A5]\n\t(No symbol) [0x001E0B3B]\n\t(No symbol) [0x0020E232]\n\t(No symbol) [0x001FA784]\n\t(No symbol) [0x0020C922]\n\t(No symbol) [0x001FA536]\n\t(No symbol) [0x001D82DC]\n\t(No symbol) [0x001D93DD]\n\tGetHandleVerifier [0x0057AABD+2539405]\n\tGetHandleVerifier [0x005BA78F+2800735]\n\tGetHandleVerifier [0x005B456C+2775612]\n\tGetHandleVerifier [0x003A51E0+616112]\n\t(No symbol) [0x002B5F8C]\n\t(No symbol) [0x002B2328]\n\t(No symbol) [0x002B240B]\n\t(No symbol) [0x002A4FF7]\n\tBaseThreadInitThunk [0x764C7D59+25]\n\tRtlInitializeExceptionChain [0x778DB74B+107]\n\tRtlClearBits [0x778DB6CF+191]\n"
     ]
    }
   ],
   "source": [
    "def css_finds(css_selector):\n",
    "    return browser.find_elements(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "def css_find(css_selector):\n",
    "    return browser.find_element(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "def find(wait, css_selector):\n",
    "    return wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, css_selector)))\n",
    "\n",
    "def finds_xpath(xpath):\n",
    "    return browser.find_elements(By.XPATH, xpath)\n",
    "\n",
    "def find_xpath(xpath):\n",
    "    return browser.find_element(By.XPATH, xpath)\n",
    "\n",
    "def find_id(id_x):\n",
    "    return browser.find_element(By.ID, id_x)\n",
    "\n",
    "# Options Setting\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('no-sandox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--start-maximized')\n",
    "options.add_argument('incognito')\n",
    "# options.add_argument('headless')\n",
    "# Header Setting\n",
    "service = Service(ChromeDriverManager().install())\n",
    "browser = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "\n",
    "# Settings\n",
    "NAVER_ID = \"sjk5838\"\n",
    "NAVER_PW = \"rlatjdwls00@K\"\n",
    "\n",
    "CAFENAME = \"1motion1\"\n",
    "BORADTITLE = \"전체글보기\"\n",
    "\n",
    "keyword = \"3D\"\n",
    "COMMENTS = \"테스트 댓글 입니다.\"\n",
    "\n",
    "# def 1\n",
    "# Crawling Start\n",
    "browser.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "browser.implicitly_wait(2)\n",
    "\n",
    "input_id = find_id('id')\n",
    "input_pw = find_id('pw')\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "pyperclip.copy(NAVER_ID) \n",
    "input_id.send_keys(Keys.CONTROL, \"v\")\n",
    "\n",
    "pyperclip.copy(NAVER_PW) \n",
    "input_pw.send_keys(Keys.CONTROL, \"v\")\n",
    "input_pw.send_keys(\"\\n\")\n",
    "\n",
    "# Not needed when it's headless\n",
    "# no_save_btn = find_id('new.dontsave')\n",
    "# no_save_btn.click()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# def 2\n",
    "browser.get(f\"https://cafe.naver.com/{CAFENAME}\")\n",
    "time.sleep(2)\n",
    "\n",
    "boardName = browser.find_element(By.LINK_TEXT, f'{BORADTITLE}')\n",
    "\n",
    "boardName.click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "browser.switch_to.frame(\"cafe_main\")\n",
    "\n",
    "browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/a').click()\n",
    "browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/ul/li[7]/a').click()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "soup = BS(browser.page_source, \"html.parser\")\n",
    "soup = soup.find_all(class_='article-board m-tcol-c')[1]\n",
    "\n",
    "datas = soup.find_all(class_='td_article')\n",
    "dates = soup.find_all(class_='td_date')\n",
    "\n",
    "a_hrefs = soup.find_all(\"a\")\n",
    "\n",
    "# def 3\n",
    "post_hrefs = []\n",
    "for href in a_hrefs:\n",
    "    if keyword in href.text:\n",
    "        post_hrefs.append(href[\"href\"])\n",
    "\n",
    "final_hrefs = []\n",
    "\n",
    "for href in post_hrefs:\n",
    "    parsed_url = urlparse(href)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    article_id = query_params['articleid'][0]\n",
    "    club_id = query_params['clubid'][0]\n",
    "    new_url = f\"https://cafe.naver.com/{CAFENAME}?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D{club_id}%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
    "    \n",
    "    final_hrefs.append(new_url)\n",
    "\n",
    "# def 4\n",
    "for r_href in final_hrefs:\n",
    "    browser.get(r_href)\n",
    "    browser.switch_to.frame(\"cafe_main\")\n",
    "    \n",
    "    text_area = find_xpath('//*[@id=\"app\"]/div/div/div[2]/div[2]/div[5]/div[2]/div[1]/textarea')\n",
    "    text_area.click()\n",
    "\n",
    "pyperclip.copy(COMMENTS) \n",
    "text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "browser.find_element(By.XPATH, '/html/body/div/div/div/div[2]/div[2]/div[5]/div[2]/div[2]/div[2]/a').click()\n",
    "\n",
    "browser.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "67e4d5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "final_hrefs = []\n",
    "\n",
    "for href in post_hrefs:\n",
    "    parsed_url = urlparse(href)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    article_id = query_params['articleid'][0]\n",
    "    club_id = query_params['clubid'][0]\n",
    "    new_url = f\"https://cafe.naver.com/1motion1?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D{club_id}%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
    "    \n",
    "    final_hrefs.append(new_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "28f2f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "for po in post_hrefs:\n",
    "    da_1 = po.split(\"&\")\n",
    "    print(da_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d1393",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = browser.find_elements(By.CSS_SELECTOR, '.comment')\n",
    "for comment in comments:\n",
    "    nickname = comment.find_element_by_css_selector('.user_name').text\n",
    "    \n",
    "    if nickname == my_nickname:\n",
    "        # 다른 URL로 넘어가는 로직 작성\n",
    "        continue\n",
    "    \n",
    "    comment_textarea = comment.find_element_by_css_selector('.comment_input textarea')\n",
    "    comment_textarea.send_keys(comment_content)\n",
    "    comment_button = comment.find_element_by_css_selector('.comment_input .comment_submit')\n",
    "    comment_button.click()\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb1e57",
   "metadata": {},
   "source": [
    "import math\n",
    "\n",
    "crawling_page = int(math.ceil(100 / 50)+1)\n",
    "print(crawling_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e15832",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb996ec",
   "metadata": {},
   "source": [
    "# Naver Cafe Macro Serach Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "8ae322be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import pyperclip\n",
    "\n",
    "def css_finds(css_selector):\n",
    "    return browser.find_elements(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "def css_find(css_selector):\n",
    "    return browser.find_element(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "def finds_xpath(xpath):\n",
    "    return browser.find_elements(By.XPATH, xpath)\n",
    "\n",
    "def find_xpath(xpath):\n",
    "    return browser.find_element(By.XPATH, xpath)\n",
    "\n",
    "def find_id(id_x):\n",
    "    return browser.find_element(By.ID, id_x)\n",
    "\n",
    "def find_classname(class_name):\n",
    "    return browser.find_element(By.CLASS_NAME, class_name)\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('no-sandox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "# options.add_argument('--start-maximized')\n",
    "options.add_argument(\"--window-size=1080,800\")\n",
    "options.add_argument('incognito')\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "browser = webdriver.Chrome(service=service, options=options)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "a5ba12a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "NAVER_ID = \"sjk5838\"\n",
    "NAVER_PW = \"rlatjdwls00@K\"\n",
    "\n",
    "CAFENAME = \"1motion1\"\n",
    "BORADTITLE = \"김성진\"\n",
    "NICKNAME = \"최고의일베충소정\"\n",
    "\n",
    "keyword = \"3D\"\n",
    "COMMENTS = \"Test Comments\"\n",
    "\n",
    "# def 1\n",
    "# Crawling Start\n",
    "\n",
    "browser.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "browser.implicitly_wait(2)\n",
    "\n",
    "input_id = find_id('id')\n",
    "input_pw = find_id('pw')\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "pyperclip.copy(NAVER_ID)\n",
    "input_id.send_keys(Keys.CONTROL, \"v\")\n",
    "\n",
    "pyperclip.copy(NAVER_PW) \n",
    "input_pw.send_keys(Keys.CONTROL, \"v\")\n",
    "input_pw.send_keys(\"\\n\")\n",
    "\n",
    "# Not needed when it's headless\n",
    "no_save_btn = find_id('new.dontsave')\n",
    "no_save_btn.click()\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "b9c1ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def 2\n",
    "browser.get(f\"https://cafe.naver.com/{CAFENAME}\")\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "5a8ece8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_input = find_id('topLayerQueryInput')\n",
    "\n",
    "pyperclip.copy(keyword)\n",
    "search_input.send_keys(Keys.CONTROL, 'v')\n",
    "search_input.send_keys('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "fbcad9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.switch_to.frame(\"cafe_main\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "1d91bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_xpath('//*[@id=\"currentSearchByTop\"]').click()\n",
    "time.sleep(1)\n",
    "find_xpath('//*[@id=\"sl_general\"]/li[2]/a').click()\n",
    "find_xpath('//*[@id=\"main-area\"]/div[1]/div[1]/form/div[4]/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "c84b489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_xpath('//*[@id=\"main-area\"]/div[1]/div[1]/form/div[4]/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "0cd68eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_xpath('//*[@id=\"listSizeSelectDiv\"]/a').click()\n",
    "find_xpath('//*[@id=\"listSizeSelectDiv\"]/ul/li[7]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "5cda014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_numbers_btn = css_finds('div.prev-next > a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "65dd5018",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_nums = []\n",
    "for i in page_numbers_btn:\n",
    "    page_nums.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "ebfe0963",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'find_all'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[506], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m soup \u001b[38;5;241m=\u001b[39m BS(browser\u001b[38;5;241m.\u001b[39mpage_source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m soup \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle-board result-board m-tcol-c\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m a_hrefs \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# def 3\u001b[39;00m\n\u001b[0;32m     38\u001b[0m post_hrefs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\element.py:2289\u001b[0m, in \u001b[0;36mResultSet.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   2288\u001b[0m     \u001b[38;5;124;03m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   2290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResultSet object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key\n\u001b[0;32m   2291\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: ResultSet object has no attribute 'find_all'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "final_hrefs = []\n",
    "\n",
    "if len(page_nums) > 1:\n",
    "    for i in page_nums[:-1]:\n",
    "        if i != 1: \n",
    "            browser.find_element(By.LINK_TEXT, f\"{i}\").click()\n",
    "            \n",
    "        soup = BS(browser.page_source, \"html.parser\")\n",
    "        soup = soup.find_all(class_='article-board result-board m-tcol-c')[0]\n",
    "\n",
    "        a_hrefs = soup.find_all(\"a\")\n",
    "\n",
    "        # def 3\n",
    "        post_hrefs = []\n",
    "        for href in a_hrefs:\n",
    "            if keyword in href.text:\n",
    "                post_hrefs.append(href[\"href\"])\n",
    "\n",
    "        for href in post_hrefs:\n",
    "            if (href == '#'):\n",
    "                pass\n",
    "            else:\n",
    "                parsed_url = urlparse(href)\n",
    "                query_params = parse_qs(parsed_url.query)\n",
    "                article_id = query_params['articleid'][0]\n",
    "                club_id = query_params['clubid'][0]\n",
    "                new_url = f\"https://cafe.naver.com/{CAFENAME}?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D{club_id}%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
    "\n",
    "                final_hrefs.append(new_url)\n",
    "        \n",
    "else:\n",
    "    soup = BS(browser.page_source, \"html.parser\")\n",
    "    soup = soup.find_all(class_='article-board result-board m-tcol-c')\n",
    "\n",
    "    a_hrefs = soup.find_all(\"a\")\n",
    "\n",
    "    # def 3\n",
    "    post_hrefs = []\n",
    "    for href in a_hrefs:\n",
    "        if keyword in href.text:\n",
    "            post_hrefs.append(href[\"href\"])\n",
    "\n",
    "    for href in post_hrefs:\n",
    "        if (href == '#'):\n",
    "            pass\n",
    "        else:\n",
    "            parsed_url = urlparse(href)\n",
    "            query_params = parse_qs(parsed_url.query)\n",
    "            article_id = query_params['articleid'][0]\n",
    "            club_id = query_params['clubid'][0]\n",
    "            new_url = f\"https://cafe.naver.com/{CAFENAME}?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D{club_id}%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
    "\n",
    "            final_hrefs.append(new_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "3ac35ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmtnicks = []\n",
    "\n",
    "# for p_href in final_hrefs:\n",
    "#     browser.get(p_href)\n",
    "#     time.sleep(1)\n",
    "#     browser.switch_to.frame(\"cafe_main\")\n",
    "#     time.sleep(1)\n",
    "    \n",
    "#     try:\n",
    "# #         nicksname = browser.find_element(By.CLASS_NAME, 'comment_inbox_name').text\n",
    "#         nickname = NICKNAME\n",
    "#         cmtNicks = browser.find_elements(By.CLASS_NAME, 'comment_nickname')\n",
    "\n",
    "#         if cmtNicks:\n",
    "#             for cmtNick in cmtNicks:\n",
    "#                 cmtnick = cmtNick.text\n",
    "#                 cmtnicks.append(cmtnick)\n",
    "\n",
    "#             if nickname in cmtnicks:\n",
    "#                 # 내 닉네임으로 댓글이 있는 경우, 다음 페이지로 넘어감\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 # 내 닉네임으로 댓글이 없는 경우, 댓글 작성\n",
    "#                 time.sleep(1)\n",
    "#                 text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "#                 text_area.click()\n",
    "\n",
    "#                 pyperclip.copy(COMMENTS) \n",
    "#                 text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "#                 register_btn = browser.find_element(By.CLASS_NAME, 'btn_register')\n",
    "#                 register_btn.click()\n",
    "#         else:\n",
    "#             # 댓글 작성\n",
    "#             time.sleep(1)\n",
    "#             text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "#             text_area.click()\n",
    "\n",
    "#             pyperclip.copy(COMMENTS) \n",
    "#             text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "#             register_btn = browser.find_element(By.CLASS_NAME, 'btn_register')\n",
    "#             register_btn.click()\n",
    "            \n",
    "#     except NoSuchElementException:\n",
    "#         pass\n",
    "\n",
    "#     cmtnicks.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55816d3",
   "metadata": {},
   "source": [
    "# Nave Cafe Macro Start Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "e3d56860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import pyperclip\n",
    "\n",
    "def css_finds(css_selector):\n",
    "    return browser.find_elements(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "def css_find(css_selector):\n",
    "    return browser.find_element(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "def finds_xpath(xpath):\n",
    "    return browser.find_elements(By.XPATH, xpath)\n",
    "\n",
    "def find_xpath(xpath):\n",
    "    return browser.find_element(By.XPATH, xpath)\n",
    "\n",
    "def find_id(id_x):\n",
    "    return browser.find_element(By.ID, id_x)\n",
    "\n",
    "def find_classname(class_name):\n",
    "    return browser.find_element(By.CLASS_NAME, class_name)\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('no-sandox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "# options.add_argument('--start-maximized')\n",
    "options.add_argument(\"--window-size=1080,800\")\n",
    "options.add_argument('incognito')\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "browser = webdriver.Chrome(service=service, options=options)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad62b728",
   "metadata": {},
   "source": [
    "### def 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "da3d2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "NAVER_ID = \"sjk5838\"\n",
    "NAVER_PW = \"rlatjdwls00@K\"\n",
    "\n",
    "CAFENAME = \"1motion1\"\n",
    "BORADTITLE = \"김성진\"\n",
    "NICKNAME = \"최고의일베충소정\"\n",
    "\n",
    "keyword = \"3D\"\n",
    "COMMENTS = \"Test Comments\"\n",
    "\n",
    "# def 1\n",
    "# Crawling Start\n",
    "\n",
    "browser.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "browser.implicitly_wait(2)\n",
    "\n",
    "input_id = find_id('id')\n",
    "input_pw = find_id('pw')\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "pyperclip.copy(NAVER_ID)\n",
    "input_id.send_keys(Keys.CONTROL, \"v\")\n",
    "\n",
    "pyperclip.copy(NAVER_PW) \n",
    "input_pw.send_keys(Keys.CONTROL, \"v\")\n",
    "input_pw.send_keys(\"\\n\")\n",
    "\n",
    "# Not needed when it's headless\n",
    "no_save_btn = find_id('new.dontsave')\n",
    "no_save_btn.click()\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba497a3",
   "metadata": {},
   "source": [
    "### def 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "174a3072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def 2\n",
    "browser.get(f\"https://cafe.naver.com/{CAFENAME}\")\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "7766fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_input = find_id('topLayerQueryInput')\n",
    "\n",
    "pyperclip.copy(keyword)\n",
    "search_input.send_keys(Keys.CONTROL, 'v')\n",
    "search_input.send_keys('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "1ab58487",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.switch_to.frame(\"cafe_main\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "79ecced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_xpath('//*[@id=\"currentSearchByTop\"]').click()\n",
    "time.sleep(1)\n",
    "find_xpath('//*[@id=\"sl_general\"]/li[2]/a').click()\n",
    "find_xpath('//*[@id=\"main-area\"]/div[1]/div[1]/form/div[4]/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "ba54d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_xpath('//*[@id=\"listSizeSelectDiv\"]/a').click()\n",
    "find_xpath('//*[@id=\"listSizeSelectDiv\"]/ul/li[7]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "e78a6793",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_numbers_btn = css_finds('div.prev-next > a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "89d28431",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_nums = []\n",
    "for i in page_numbers_btn:\n",
    "    page_nums.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "715e9b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hrefs = []\n",
    "\n",
    "if len(page_nums) > 1:\n",
    "    for i in page_nums[:-1]:\n",
    "        if i != 1: \n",
    "            browser.find_element(By.LINK_TEXT, f\"{i}\").click()\n",
    "            \n",
    "        soup = BS(browser.page_source, \"html.parser\")\n",
    "        soup = soup.find_all(class_='article-board m-tcol-c')[1]\n",
    "\n",
    "        a_hrefs = soup.find_all(\"a\")\n",
    "\n",
    "        # def 3\n",
    "        post_hrefs = []\n",
    "        for href in a_hrefs:\n",
    "            if keyword in href.text:\n",
    "                post_hrefs.append(href[\"href\"])\n",
    "\n",
    "        for href in post_hrefs:\n",
    "            if (href == '#'):\n",
    "                pass\n",
    "            else:\n",
    "                parsed_url = urlparse(href)\n",
    "                query_params = parse_qs(parsed_url.query)\n",
    "                article_id = query_params['articleid'][0]\n",
    "                club_id = query_params['clubid'][0]\n",
    "                new_url = f\"https://cafe.naver.com/{CAFENAME}?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D{club_id}%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
    "\n",
    "                final_hrefs.append(new_url)\n",
    "        \n",
    "else:\n",
    "    soup = BS(browser.page_source, \"html.parser\")\n",
    "    soup = soup.find_all(class_='article-board m-tcol-c')[1]\n",
    "\n",
    "    a_hrefs = soup.find_all(\"a\")\n",
    "\n",
    "    # def 3\n",
    "    post_hrefs = []\n",
    "    for href in a_hrefs:\n",
    "        if keyword in href.text:\n",
    "            post_hrefs.append(href[\"href\"])\n",
    "\n",
    "    for href in post_hrefs:\n",
    "        if (href == '#'):\n",
    "            pass\n",
    "        else:\n",
    "            parsed_url = urlparse(href)\n",
    "            query_params = parse_qs(parsed_url.query)\n",
    "            article_id = query_params['articleid'][0]\n",
    "            club_id = query_params['clubid'][0]\n",
    "            new_url = f\"https://cafe.naver.com/{CAFENAME}?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D{club_id}%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
    "\n",
    "            final_hrefs.append(new_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "ef14fb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f93bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9a4c29b",
   "metadata": {},
   "source": [
    "### def 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "8258b4d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=114.0.5735.134)\nStacktrace:\nBacktrace:\n\tGetHandleVerifier [0x0041A813+48355]\n\t(No symbol) [0x003AC4B1]\n\t(No symbol) [0x002B5358]\n\t(No symbol) [0x0029D293]\n\t(No symbol) [0x002FE37B]\n\t(No symbol) [0x0030C473]\n\t(No symbol) [0x002FA536]\n\t(No symbol) [0x002D82DC]\n\t(No symbol) [0x002D93DD]\n\tGetHandleVerifier [0x0067AABD+2539405]\n\tGetHandleVerifier [0x006BA78F+2800735]\n\tGetHandleVerifier [0x006B456C+2775612]\n\tGetHandleVerifier [0x004A51E0+616112]\n\t(No symbol) [0x003B5F8C]\n\t(No symbol) [0x003B2328]\n\t(No symbol) [0x003B240B]\n\t(No symbol) [0x003A4FF7]\n\tBaseThreadInitThunk [0x76767D59+25]\n\tRtlInitializeExceptionChain [0x77E9B74B+107]\n\tRtlClearBits [0x77E9B6CF+191]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[560], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     browser\u001b[38;5;241m.\u001b[39mget(p_href)\n\u001b[0;32m      5\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mbrowser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswitch_to\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcafe_main\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#         nicksname = browser.find_element(By.CLASS_NAME, 'comment_inbox_name').text\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\switch_to.py:85\u001b[0m, in \u001b[0;36mSwitchTo.frame\u001b[1;34m(self, frame_reference)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(frame_reference, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m         frame_reference \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_reference\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m NoSuchElementException:\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:740\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    737\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    738\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 740\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:346\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    344\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:245\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    243\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=114.0.5735.134)\nStacktrace:\nBacktrace:\n\tGetHandleVerifier [0x0041A813+48355]\n\t(No symbol) [0x003AC4B1]\n\t(No symbol) [0x002B5358]\n\t(No symbol) [0x0029D293]\n\t(No symbol) [0x002FE37B]\n\t(No symbol) [0x0030C473]\n\t(No symbol) [0x002FA536]\n\t(No symbol) [0x002D82DC]\n\t(No symbol) [0x002D93DD]\n\tGetHandleVerifier [0x0067AABD+2539405]\n\tGetHandleVerifier [0x006BA78F+2800735]\n\tGetHandleVerifier [0x006B456C+2775612]\n\tGetHandleVerifier [0x004A51E0+616112]\n\t(No symbol) [0x003B5F8C]\n\t(No symbol) [0x003B2328]\n\t(No symbol) [0x003B240B]\n\t(No symbol) [0x003A4FF7]\n\tBaseThreadInitThunk [0x76767D59+25]\n\tRtlInitializeExceptionChain [0x77E9B74B+107]\n\tRtlClearBits [0x77E9B6CF+191]\n"
     ]
    }
   ],
   "source": [
    "cmtnicks = []\n",
    "\n",
    "for p_href in final_hrefs:\n",
    "    browser.get(p_href)\n",
    "    time.sleep(1)\n",
    "    browser.switch_to.frame(\"cafe_main\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    try:\n",
    "#         nicksname = browser.find_element(By.CLASS_NAME, 'comment_inbox_name').text\n",
    "        nickname = NICKNAME\n",
    "        cmtNicks = browser.find_elements(By.CLASS_NAME, 'comment_nickname')\n",
    "\n",
    "        if cmtNicks:\n",
    "            for cmtNick in cmtNicks:\n",
    "                cmtnick = cmtNick.text\n",
    "                cmtnicks.append(cmtnick)\n",
    "\n",
    "            if nickname in cmtnicks:\n",
    "                # 내 닉네임으로 댓글이 있는 경우, 다음 페이지로 넘어감\n",
    "                continue\n",
    "            else:\n",
    "                # 내 닉네임으로 댓글이 없는 경우, 댓글 작성\n",
    "                time.sleep(1)\n",
    "                text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "                text_area.click()\n",
    "\n",
    "                pyperclip.copy(COMMENTS) \n",
    "                text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "                register_btn = browser.find_element(By.CLASS_NAME, 'btn_register')\n",
    "                register_btn.click()\n",
    "        else:\n",
    "            # 댓글 작성\n",
    "            time.sleep(1)\n",
    "            text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "            text_area.click()\n",
    "\n",
    "            pyperclip.copy(COMMENTS) \n",
    "            text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "            register_btn = browser.find_element(By.CLASS_NAME, 'btn_register')\n",
    "            register_btn.click()\n",
    "            \n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "    cmtnicks.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61501d40",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd09e5",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51f9fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmtnicks = []\n",
    "\n",
    "for p_href in final_hrefs:\n",
    "    browser.get(p_href)\n",
    "    time.sleep(1)\n",
    "    browser.switch_to.frame(\"cafe_main\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    nickname = NICKNAME\n",
    "\n",
    "    cmtNicks = browser.find_elements(By.CLASS_NAME, 'comment_nickname')\n",
    "    \n",
    "    if cmtNicks:\n",
    "        for cmtNick in cmtNicks:\n",
    "            cmtnick = cmtNick.text\n",
    "            cmtnicks.append(cmtnick)\n",
    "\n",
    "        if nickname in cmtnicks:\n",
    "            # 내 닉네임으로 댓글이 있는 경우, 다음 페이지로 넘어감\n",
    "            continue\n",
    "        else:\n",
    "            # 내 닉네임으로 댓글이 없는 경우, 댓글 작성\n",
    "            time.sleep(1)\n",
    "            text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "            text_area.click()\n",
    "\n",
    "            pyperclip.copy(COMMENTS) \n",
    "            text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "            browser.find_element(By.XPATH, '/html/body/div/div/div/div[2]/div[2]/div[5]/div[2]/div[2]/div[2]/a').click()\n",
    "    else:\n",
    "        # 댓글 작성\n",
    "        time.sleep(1)\n",
    "        text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "        text_area.click()\n",
    "\n",
    "        pyperclip.copy(COMMENTS) \n",
    "        text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "        browser.find_element(By.XPATH, '/html/body/div/div/div/div[2]/div[2]/div[5]/div[2]/div[2]/div[2]/a').click()\n",
    "\n",
    "    cmtnicks.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0241bb7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (236959528.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[61], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    def crawling_start(NAVER_ID, NAVER_PW, CAFENAME, keyword = \"전체글보기\", COMMENTS):\u001b[0m\n\u001b[1;37m                                                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def crawling_start(NAVER_ID, NAVER_PW, CAFENAME, keyword, COMMENTS):\n",
    "    def selenuim_setting():\n",
    "        # Options Setting\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('no-sandox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--start-maximized')\n",
    "        options.add_argument('incognito')\n",
    "        # options.add_argument('headless')\n",
    "        # Header Setting\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        browser = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        return browser\n",
    "\n",
    "    def naverMarco(browser):\n",
    "        def css_finds(css_selector):\n",
    "            return browser.find_elements(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "        def css_find(css_selector):\n",
    "            return browser.find_element(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "        def find(wait, css_selector):\n",
    "            return wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, css_selector)))\n",
    "\n",
    "        def finds_xpath(xpath):\n",
    "            return browser.find_elements(By.XPATH, xpath)\n",
    "\n",
    "        def find_xpath(xpath):\n",
    "            return browser.find_element(By.XPATH, xpath)\n",
    "\n",
    "        def find_id(id_x):\n",
    "            return browser.find_element(By.ID, id_x)\n",
    "\n",
    "        \n",
    "        browser.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "        browser.implicitly_wait(2)\n",
    "\n",
    "        input_id = find_id('id')\n",
    "        input_pw = find_id('pw')\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        pyperclip.copy(NAVER_ID) \n",
    "        input_id.send_keys(Keys.CONTROL, \"v\")\n",
    "\n",
    "        pyperclip.copy(NAVER_PW) \n",
    "        input_pw.send_keys(Keys.CONTROL, \"v\")\n",
    "        input_pw.send_keys(\"\\n\")\n",
    "\n",
    "        time.sleep(1)\n",
    "        \n",
    "        browser.get(f\"https://cafe.naver.com/{CAFENAME}\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "\n",
    "        boardName = browser.find_element(By.LINK_TEXT, f'{BORADTITLE}')\n",
    "        boardName.click()\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        browser.switch_to.frame(\"cafe_main\")\n",
    "\n",
    "        browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/a').click()\n",
    "        browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/ul/li[7]/a').click()\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        soup = BS(browser.page_source, \"html.parser\")\n",
    "        soup = soup.find_all(class_='article-board m-tcol-c')[1]\n",
    "\n",
    "        datas = soup.find_all(class_='td_article')\n",
    "        dates = soup.find_all(class_='td_date')\n",
    "\n",
    "        a_hrefs = soup.find_all(\"a\")\n",
    "        \n",
    "        \n",
    "        post_hrefs = []\n",
    "        \n",
    "        for href in a_hrefs:\n",
    "            if keyword in href.text:\n",
    "                post_hrefs.append(href[\"href\"])\n",
    "\n",
    "                \n",
    "        final_hrefs = []\n",
    "\n",
    "        for href in post_hrefs:\n",
    "            parsed_url = urlparse(href)\n",
    "            query_params = parse_qs(parsed_url.query)\n",
    "            article_id = query_params['articleid'][0]\n",
    "            club_id = query_params['clubid'][0]\n",
    "            new_url = f\"https://cafe.naver.com/{CAFENAME}?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D{club_id}%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
    "\n",
    "            final_hrefs.append(new_url)\n",
    "\n",
    "            \n",
    "        cmtnicks = []\n",
    "\n",
    "        for p_href in final_hrefs:\n",
    "            browser.get(p_href)\n",
    "            time.sleep(1)\n",
    "            browser.switch_to.frame(\"cafe_main\")\n",
    "            time.sleep(1)\n",
    "\n",
    "            try:\n",
    "                nicksname = browser.find_element(By.CLASS_NAME, 'comment_inbox_name').text\n",
    "                cmtNicks = browser.find_elements(By.CLASS_NAME, 'comment_nickname')\n",
    "\n",
    "                if cmtNicks:\n",
    "                    for cmtNick in cmtNicks:\n",
    "                        cmtnick = cmtNick.text\n",
    "                        cmtnicks.append(cmtnick)\n",
    "\n",
    "                    if nickname in cmtnicks:\n",
    "                        # 내 닉네임으로 댓글이 있는 경우, 다음 페이지로 넘어감\n",
    "                        continue\n",
    "                    else:\n",
    "                        # 내 닉네임으로 댓글이 없는 경우, 댓글 작성\n",
    "                        time.sleep(1)\n",
    "                        text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "                        text_area.click()\n",
    "\n",
    "                        pyperclip.copy(COMMENTS) \n",
    "                        text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "                        register_btn = browser.find_element(By.CLASS_NAME, 'btn_register')\n",
    "                        register_btn.click()\n",
    "                else:\n",
    "                    # 댓글 작성\n",
    "                    time.sleep(1)\n",
    "                    text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "                    text_area.click()\n",
    "\n",
    "                    pyperclip.copy(COMMENTS) \n",
    "                    text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "                    register_btn = browser.find_element(By.CLASS_NAME, 'btn_register')\n",
    "                    register_btn.click()\n",
    "\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "\n",
    "            cmtnicks.clear()\n",
    "            \n",
    "            df = pd.DataFrame(final_hrefs)\n",
    "            \n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f58bd8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cmtNicks:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ce49c6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cafe.naver.com/1motion1?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D28520242%2526page%3D1%2526boardtype%3DL%2526articleid%3D559%2526referrerAllArticles%3Dtrue\n"
     ]
    }
   ],
   "source": [
    "print(final_hrefs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86789230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "37689eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C://Users//user//Downloads//test_travel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c2f96ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7d14e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fd4cc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['연락처'].str.contains(\"\\+82\" , na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c290cc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21    박영준 (Brad Park)\n",
       "67                이호철\n",
       "71                정소정\n",
       "Name: 이름, dtype: object"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['이름']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "adaafff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "010-8262-6228\n",
      "010-9612-2352\n",
      "010-3654-1504\n"
     ]
    }
   ],
   "source": [
    "for i in df2['연락처']:\n",
    "    tt = i.split(\"+82\")[1]\n",
    "    t2 = tt.split(\"휴대폰\")[0]\n",
    "    t3 = '0'+t2[1:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56d1095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import pyperclip\n",
    "\n",
    "\n",
    "def naverCafeMarco(NAVER_ID, NAVER_PW, CAFENAME, BORADTITLE, NICKNAME, keyword, COMMENTS):\n",
    "\n",
    "    def css_finds(css_selector):\n",
    "        return browser.find_elements(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "    def css_find(css_selector):\n",
    "        return browser.find_element(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "    def find(wait, css_selector):\n",
    "        return wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, css_selector)))\n",
    "\n",
    "    def finds_xpath(xpath):\n",
    "        return browser.find_elements(By.XPATH, xpath)\n",
    "\n",
    "    def find_xpath(xpath):\n",
    "        return browser.find_element(By.XPATH, xpath)\n",
    "\n",
    "    def find_id(id_x):\n",
    "        return browser.find_element(By.ID, id_x)\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('no-sandox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--start-maximized')\n",
    "    options.add_argument('incognito')\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    browser = webdriver.Chrome(service=service, options=options)    \n",
    "\n",
    "    # def 1\n",
    "    # Crawling Start\n",
    "\n",
    "    browser.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "    browser.implicitly_wait(2)\n",
    "\n",
    "    input_id = find_id('id')\n",
    "    input_pw = find_id('pw')\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    pyperclip.copy(NAVER_ID)\n",
    "    input_id.send_keys(Keys.CONTROL, \"v\")\n",
    "\n",
    "    pyperclip.copy(NAVER_PW) \n",
    "    input_pw.send_keys(Keys.CONTROL, \"v\")\n",
    "    input_pw.send_keys(\"\\n\")\n",
    "\n",
    "    # Not needed when it's headless\n",
    "    # no_save_btn = find_id('new.dontsave')\n",
    "    # no_save_btn.click()\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    # def 2\n",
    "    browser.get(f\"https://cafe.naver.com/{CAFENAME}\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    boardName = browser.find_element(By.LINK_TEXT, f'{BORADTITLE}')\n",
    "\n",
    "    boardName.click()\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    browser.switch_to.frame(\"cafe_main\")\n",
    "\n",
    "    browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/a').click()\n",
    "    browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/ul/li[7]/a').click()\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    soup = BS(browser.page_source, \"html.parser\")\n",
    "    soup = soup.find_all(class_='article-board m-tcol-c')[1]\n",
    "\n",
    "    datas = soup.find_all(class_='td_article')\n",
    "    dates = soup.find_all(class_='td_date')\n",
    "\n",
    "    a_hrefs = soup.find_all(\"a\")\n",
    "\n",
    "    # def 3\n",
    "    post_hrefs = []\n",
    "    for href in a_hrefs:\n",
    "        if keyword in href.text:\n",
    "            post_hrefs.append(href[\"href\"])\n",
    "\n",
    "    final_hrefs = []\n",
    "\n",
    "    for href in post_hrefs:\n",
    "        parsed_url = urlparse(href)\n",
    "        query_params = parse_qs(parsed_url.query)\n",
    "        article_id = query_params['articleid'][0]\n",
    "        club_id = query_params['clubid'][0]\n",
    "        new_url = f\"https://cafe.naver.com/{CAFENAME}?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D{club_id}%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
    "\n",
    "        final_hrefs.append(new_url)\n",
    "\n",
    "    while len(final_hrefs) == 0:\n",
    "        # final_hrefs가 빈 리스트인 경우, 계속해서 반복적으로 돌립니다.\n",
    "        time.sleep(1)\n",
    "        browser.refresh()\n",
    "\n",
    "        boardName = browser.find_element(By.LINK_TEXT, f'{BORADTITLE}')\n",
    "\n",
    "        boardName.click()\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        browser.switch_to.frame(\"cafe_main\")\n",
    "\n",
    "        browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/a').click()\n",
    "        browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/ul/li[7]/a').click()\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        soup = BS(browser.page_source, \"html.parser\")\n",
    "        soup = soup.find_all(class_='article-board m-tcol-c')[1]\n",
    "\n",
    "        datas = soup.find_all(class_='td_article')\n",
    "        dates = soup.find_all(class_='td_date')\n",
    "\n",
    "        a_hrefs = soup.find_all(\"a\")\n",
    "\n",
    "        # def 3\n",
    "        post_hrefs = []\n",
    "        for href in a_hrefs:\n",
    "            if keyword in href.text:\n",
    "                post_hrefs.append(href[\"href\"])\n",
    "\n",
    "        final_hrefs = []\n",
    "\n",
    "        for href in post_hrefs:\n",
    "            parsed_url = urlparse(href)\n",
    "            query_params = parse_qs(parsed_url.query)\n",
    "            article_id = query_params['articleid'][0]\n",
    "            club_id = query_params['clubid'][0]\n",
    "            new_url = f\"https://cafe.naver.com/{CAFENAME}?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D{club_id}%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
    "\n",
    "            final_hrefs.append(new_url)\n",
    "            print(\"해당 키워드를 가진 게시물을 찾았습니다.\")\n",
    "\n",
    "\n",
    "    cmtnicks = []\n",
    "\n",
    "    for p_href in final_hrefs:\n",
    "        browser.get(p_href)\n",
    "        time.sleep(1)\n",
    "        browser.switch_to.frame(\"cafe_main\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "    #         nicksname = browser.find_element(By.CLASS_NAME, 'comment_inbox_name').text\n",
    "            nickname = NICKNAME\n",
    "            cmtNicks = browser.find_elements(By.CLASS_NAME, 'comment_nickname')\n",
    "\n",
    "            if cmtNicks:\n",
    "                for cmtNick in cmtNicks:\n",
    "                    cmtnick = cmtNick.text\n",
    "                    cmtnicks.append(cmtnick)\n",
    "\n",
    "                if nickname in cmtnicks:\n",
    "                    # 내 닉네임으로 댓글이 있는 경우, 다음 페이지로 넘어감\n",
    "                    print(\"해당 게시물은 이미 댓글을 작성한 적이 있는 게시물 이므로 다음 페이지로 넘어갑니다.\")\n",
    "                    continue\n",
    "                else:\n",
    "                    # 내 닉네임으로 댓글이 없는 경우, 댓글 작성\n",
    "                    time.sleep(1)\n",
    "                    text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "                    text_area.click()\n",
    "\n",
    "                    pyperclip.copy(COMMENTS)\n",
    "                    text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "                    register_btn = browser.find_element(By.CLASS_NAME, 'btn_register')\n",
    "                    register_btn.click()\n",
    "                    print(\"새로운 댓글을 작성 하였습니다.\")\n",
    "            else:\n",
    "                # 댓글 작성\n",
    "                time.sleep(1)\n",
    "                text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "                text_area.click()\n",
    "\n",
    "                pyperclip.copy(COMMENTS) \n",
    "                text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "                register_btn = browser.find_element(By.CLASS_NAME, 'btn_register')\n",
    "                register_btn.click()\n",
    "                print(\"새로운 댓글을 작성 하였습니다.\")\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        cmtnicks.clear()\n",
    "        print(\"매크로를 종료합니다.\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "    browser.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "2557a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAVER_ID = \"sjk5838\"\n",
    "NAVER_PW = \"rlatjdwls00@K\"\n",
    "\n",
    "CAFENAME = \"1motion1\"\n",
    "BORADTITLE = \"전체글보기\"\n",
    "NICKNAME = \"최고의일베충소정\"\n",
    "\n",
    "keyword = \"3D\"\n",
    "COMMENTS = \"Test Comments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "973f6c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "해당 게시물은 이미 댓글을 작성한 적이 있는 게시물 이므로 다음 페이지로 넘어갑니다.\n",
      "해당 게시물은 이미 댓글을 작성한 적이 있는 게시물 이므로 다음 페이지로 넘어갑니다.\n"
     ]
    }
   ],
   "source": [
    "naverCafeMarco(NAVER_ID, NAVER_PW, CAFENAME, BORADTITLE, NICKNAME, keyword, COMMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56aa099d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://cafe.naver.com/1motion1?iframe_url_utf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://cafe.naver.com/1motion1?iframe_url_utf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  https://cafe.naver.com/1motion1?iframe_url_utf...\n",
       "1  https://cafe.naver.com/1motion1?iframe_url_utf..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(final_hrefs)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "a72bda77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C://Users//user//Downloads//음식_2023-06-26_145817_utf-8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "d594e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C://Users//user//Downloads//음식_2023-06-26_145817_utf-8.csv\")\n",
    "\n",
    "df = df.fillna('null')\n",
    "\n",
    "numbers = df['연락처'].str.split('년').str.get(0).str[-4:]\n",
    "int_values = []\n",
    "for item in numbers:\n",
    "    if item.isdigit():\n",
    "        int_values.append(int(item))\n",
    "    else:\n",
    "        int_values.append('없음')\n",
    "        \n",
    "df.isnull().sum()\n",
    "df = df.fillna('null')\n",
    "numbers = df['연락처'].str.split('년').str.get(0).str[-4:]\n",
    "int_values = []\n",
    "for item in numbers:\n",
    "    if item.isdigit():\n",
    "        int_values.append(int(item))\n",
    "    else:\n",
    "        int_values.append('없음')\n",
    "        \n",
    "df['출생연도'] = int_values\n",
    "\n",
    "df.columns = ['번호', '이름', '연락처', '지역', '출생연도']\n",
    "\n",
    "df['지역'] = df['지역'].str.split('이전 거주지').str.get(1).str.split('현재').str.get(0).fillna('표시할 장소 없음')\n",
    "\n",
    "if len(df[df['연락처'].str.contains('\\+82')]) >= 1:\n",
    "    df['연락처'] = df['연락처'].str.split('\\+82 ').str.get(1).str[0:12].fillna('없음')\n",
    "    \n",
    "df.index[df['연락처'].str.contains('1')]\n",
    "    \n",
    "phone_num = df.index[df['연락처'].str.contains('1')]\n",
    "df.loc[phone_num, '연락처'] = \"0\" + df.loc[phone_num, '연락처']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "7b3bfbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna('null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "39ca8cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = df['연락처'].str.split('년').str.get(0).str[-4:]\n",
    "int_values = []\n",
    "for item in numbers:\n",
    "    if item.isdigit():\n",
    "        int_values.append(int(item))\n",
    "    else:\n",
    "        int_values.append('없음')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "4a44dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "df = df.fillna('null')\n",
    "numbers = df['연락처'].str.split('년').str.get(0).str[-4:]\n",
    "int_values = []\n",
    "for item in numbers:\n",
    "    if item.isdigit():\n",
    "        int_values.append(int(item))\n",
    "    else:\n",
    "        int_values.append('없음')\n",
    "df['출생연도'] = int_values\n",
    "df.columns = ['번호', '이름', '연락처', '지역', '출생연도']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "0e0df0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['연락처'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "3f7806ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['지역'] = df['지역'].str.split('이전 거주지').str.get(1).str.split('현재').str.get(0).fillna('표시할 장소 없음')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "89840ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([2], dtype='int64')"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['지역'] = df['지역'].str.split('이전 거주지').str.get(1).str.split('현재').str.get(0).fillna('표시할 장소 없음')\n",
    "\n",
    "if len(df[df['연락처'].str.contains('\\+82')]) >= 1:\n",
    "    df['연락처'] = df['연락처'].str.split('\\+82 ').str.get(1).str[0:12].fillna('없음')\n",
    "    \n",
    "df.index[df['연락처'].str.contains('1')]\n",
    "    \n",
    "phone_num = df.index[df['연락처'].str.contains('1')]\n",
    "df.loc[phone_num, '연락처'] = \"0\" + df.loc[phone_num, '연락처']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "7fe0094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_num = df.index[df['연락처'].str.contains('1')]\n",
    "df.loc[phone_num, '연락처'] = \"0\" + df.loc[phone_num, '연락처']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "fd86d8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    011-638-1205휴\n",
       "Name: 연락처, dtype: object"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[phone_num, '연락처']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "9705c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_numbers = []\n",
    "\n",
    "for i in df['연락처']:\n",
    "    if \"10-\" in i:\n",
    "        ii = \"0\" + i[1:]\n",
    "        int_numbers.append(ii)\n",
    "    else:\n",
    "        int_numbers.append('없음')\n",
    "df['연락처'] = int_numbers\n",
    "#         df.to_csv(f\"{f_name}_cp949.csv\", encoding='CP949')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e5af67",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710daebf",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "3597e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import pyperclip\n",
    "\n",
    "def css_finds(css_selector):\n",
    "    return browser.find_elements(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "def css_find(css_selector):\n",
    "    return browser.find_element(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "def find(wait, css_selector):\n",
    "    return wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, css_selector)))\n",
    "\n",
    "def finds_xpath(xpath):\n",
    "    return browser.find_elements(By.XPATH, xpath)\n",
    "\n",
    "def find_xpath(xpath):\n",
    "    return browser.find_element(By.XPATH, xpath)\n",
    "\n",
    "def find_id(id_x):\n",
    "    return browser.find_element(By.ID, id_x)\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('no-sandox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--start-maximized')\n",
    "options.add_argument('incognito')\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "browser = webdriver.Chrome(service=service, options=options)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "3c8399f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def 1\n",
    "# Crawling Start\n",
    "\n",
    "browser.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "browser.implicitly_wait(2)\n",
    "\n",
    "input_id = find_id('id')\n",
    "input_pw = find_id('pw')\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "pyperclip.copy(NAVER_ID)\n",
    "input_id.send_keys(Keys.CONTROL, \"v\")\n",
    "\n",
    "pyperclip.copy(NAVER_PW) \n",
    "input_pw.send_keys(Keys.CONTROL, \"v\")\n",
    "input_pw.send_keys(\"\\n\")\n",
    "\n",
    "# Not needed when it's headless\n",
    "no_save_btn = find_id('new.dontsave')\n",
    "no_save_btn.click()\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "36b56726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def 2\n",
    "browser.get(f\"https://cafe.naver.com/{CAFENAME}\")\n",
    "time.sleep(2)\n",
    "\n",
    "boardName = browser.find_element(By.LINK_TEXT, f'{BORADTITLE}')\n",
    "\n",
    "boardName.click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "browser.switch_to.frame(\"cafe_main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "2ebf0bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/a').click()\n",
    "browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/ul/li[7]/a').click()\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "8f9527d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_numbers_btn = css_finds('div.prev-next > a')\n",
    "\n",
    "page_nums = []\n",
    "for i in page_numbers_btn:\n",
    "    page_nums.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "4edec4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "final_hrefs = []\n",
    "\n",
    "if len(page_nums) > 1:\n",
    "    for i in range(1, 4):\n",
    "        browser.find_element(By.LINK_TEXT, f\"{i}\").click()\n",
    "        \n",
    "        soup = BS(browser.page_source, \"html.parser\")\n",
    "        soup = soup.find_all(class_='article-board m-tcol-c')[1]\n",
    "\n",
    "        datas = soup.find_all(class_='td_article')\n",
    "        dates = soup.find_all(class_='td_date')\n",
    "\n",
    "        a_hrefs = soup.find_all(\"a\")\n",
    "        \n",
    "        # def 3\n",
    "        post_hrefs = []\n",
    "        for href in a_hrefs:\n",
    "            if keyword in href.text:\n",
    "                post_hrefs.append(href[\"href\"])\n",
    "\n",
    "        for href in post_hrefs:\n",
    "            parsed_url = urlparse(href)\n",
    "            query_params = parse_qs(parsed_url.query)\n",
    "            article_id = query_params['articleid'][0]\n",
    "            club_id = query_params['clubid'][0]\n",
    "            new_url = f\"https://cafe.naver.com/{CAFENAME}?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D{club_id}%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
    "\n",
    "            final_hrefs.append(new_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "7b107917",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "soup = BS(browser.page_source, \"html.parser\")\n",
    "soup = soup.find_all(class_='article-board m-tcol-c')[1]\n",
    "\n",
    "datas = soup.find_all(class_='td_article')\n",
    "dates = soup.find_all(class_='td_date')\n",
    "\n",
    "a_hrefs = soup.find_all(\"a\")\n",
    "\n",
    "# def 3\n",
    "post_hrefs = []\n",
    "for href in a_hrefs:\n",
    "    if keyword in href.text:\n",
    "        post_hrefs.append(href[\"href\"])\n",
    "\n",
    "final_hrefs = []\n",
    "\n",
    "for href in post_hrefs:\n",
    "    parsed_url = urlparse(href)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    article_id = query_params['articleid'][0]\n",
    "    club_id = query_params['clubid'][0]\n",
    "    new_url = f\"https://cafe.naver.com/{CAFENAME}?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D{club_id}%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
    "\n",
    "    final_hrefs.append(new_url)\n",
    "\n",
    "while len(final_hrefs) == 0:\n",
    "    # If final_hrefs is an empty list, continue to rotate repeatedly.\n",
    "    time.sleep(1)\n",
    "    browser.refresh()\n",
    "\n",
    "    boardName = browser.find_element(By.LINK_TEXT, f'{BORADTITLE}')\n",
    "\n",
    "    boardName.click()\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    browser.switch_to.frame(\"cafe_main\")\n",
    "\n",
    "    browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/a').click()\n",
    "    browser.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/ul/li[7]/a').click()\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    soup = BS(browser.page_source, \"html.parser\")\n",
    "    soup = soup.find_all(class_='article-board m-tcol-c')[1]\n",
    "\n",
    "    datas = soup.find_all(class_='td_article')\n",
    "    dates = soup.find_all(class_='td_date')\n",
    "\n",
    "    a_hrefs = soup.find_all(\"a\")\n",
    "\n",
    "    # def 3\n",
    "    post_hrefs = []\n",
    "    for href in a_hrefs:\n",
    "        if keyword in href.text:\n",
    "            post_hrefs.append(href[\"href\"])\n",
    "\n",
    "    final_hrefs = []\n",
    "\n",
    "    for href in post_hrefs:\n",
    "        parsed_url = urlparse(href)\n",
    "        query_params = parse_qs(parsed_url.query)\n",
    "        article_id = query_params['articleid'][0]\n",
    "        club_id = query_params['clubid'][0]\n",
    "        new_url = f\"https://cafe.naver.com/{CAFENAME}?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D{club_id}%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
    "\n",
    "        final_hrefs.append(new_url)\n",
    "\n",
    "    soup = BS(browser.page_source, \"html.parser\")\n",
    "    soup = soup.find_all(class_='article-board m-tcol-c')[1]\n",
    "\n",
    "    datas = soup.find_all(class_='td_article')\n",
    "    dates = soup.find_all(class_='td_date')\n",
    "\n",
    "    a_hrefs = soup.find_all(\"a\")\n",
    "\n",
    "    post_hrefs = []\n",
    "    for href in a_hrefs:\n",
    "        if keyword in href.text:\n",
    "            post_hrefs.append(href[\"href\"])\n",
    "\n",
    "    final_hrefs = []\n",
    "\n",
    "    for href in post_hrefs:\n",
    "        parsed_url = urlparse(href)\n",
    "        query_params = parse_qs(parsed_url.query)\n",
    "        article_id = query_params['articleid'][0]\n",
    "        club_id = query_params['clubid'][0]\n",
    "        new_url = f\"https://cafe.naver.com/{CAFENAME}?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D{club_id}%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
    "\n",
    "        final_hrefs.append(new_url)\n",
    "\n",
    "\n",
    "cmtnicks = []\n",
    "\n",
    "for p_href in final_hrefs:\n",
    "    browser.get(p_href)\n",
    "    time.sleep(1)\n",
    "    browser.switch_to.frame(\"cafe_main\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    try:\n",
    "#         nicksname = browser.find_element(By.CLASS_NAME, 'comment_inbox_name').text\n",
    "        nickname = NICKNAME\n",
    "        cmtNicks = browser.find_elements(By.CLASS_NAME, 'comment_nickname')\n",
    "\n",
    "        if cmtNicks:\n",
    "            for cmtNick in cmtNicks:\n",
    "                cmtnick = cmtNick.text\n",
    "                cmtnicks.append(cmtnick)\n",
    "\n",
    "            if nickname in cmtnicks:\n",
    "                continue\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "                text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "                text_area.click()\n",
    "\n",
    "                pyperclip.copy(COMMENTS) \n",
    "                text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "                register_btn = browser.find_element(By.CLASS_NAME, 'btn_register')\n",
    "                register_btn.click()\n",
    "        else:\n",
    "            # Write Comment\n",
    "            time.sleep(1)\n",
    "            text_area = browser.find_element(By.CLASS_NAME, 'comment_inbox_text')\n",
    "            text_area.click()\n",
    "\n",
    "            pyperclip.copy(COMMENTS) \n",
    "            text_area.send_keys(Keys.CONTROL, \"v\")\n",
    "            register_btn = browser.find_element(By.CLASS_NAME, 'btn_register')\n",
    "            register_btn.click()\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "    cmtnicks.clear()\n",
    "\n",
    "time.sleep(3)\n",
    "browser.close()\n",
    "\n",
    "title = []\n",
    "\n",
    "for da in datas:\n",
    "    art_title = da.find(class_='article')\n",
    "    art_title.get_text().strip()\n",
    "    title.append(art_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf47ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
