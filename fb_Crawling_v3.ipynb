{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18cb75e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver_manager in c:\\users\\user\\anaconda3\\lib\\site-packages (3.8.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from webdriver_manager) (4.64.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\user\\anaconda3\\lib\\site-packages (from webdriver_manager) (1.0.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from webdriver_manager) (2.28.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from webdriver_manager) (22.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2023.5.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm->webdriver_manager) (0.4.6)\n",
      "Requirement already satisfied: selenium in c:\\users\\user\\anaconda3\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (0.10.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (2023.5.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: BeautifulSoup4 in c:\\users\\user\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from BeautifulSoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: pyperclip in c:\\users\\user\\anaconda3\\lib\\site-packages (1.8.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver_manager\n",
    "!pip install selenium\n",
    "!pip install BeautifulSoup4\n",
    "!pip install pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "251ae33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import time\n",
    "import pyperclip\n",
    "\n",
    "def selenium_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--proxy-server=socks5://127.0.0.1:9150\")\n",
    "    options.add_argument('window-size=1920x1080')\n",
    "    options.add_argument('disable-gpu')\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('no-sandox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--start-maximized')\n",
    "    options.add_argument('incognito')\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    browser = webdriver.Chrome(service=service, options=options)\n",
    "    return browser\n",
    "                               \n",
    "if __name__ == \"__main__\":\n",
    "    browser = selenium_driver()\n",
    "    try:\n",
    "        #open browser\n",
    "        browser.get(\"https://www.facebook.com/\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        #log info, search keyword\n",
    "        USER = \"shm8485@gmail.com\"\n",
    "        PWD = \"sohee8485\"\n",
    "        KEYWORD = \"여행\\n\"\n",
    "        \n",
    "        #login\n",
    "        elem_id = browser.find_element(\"id\", \"email\")\n",
    "        pyperclip.copy(USER)\n",
    "        elem_id.send_keys(Keys.CONTROL, \"v\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        elem_pw = browser.find_element(\"id\", \"pass\")\n",
    "        pyperclip.copy(PWD)\n",
    "        elem_pw.send_keys(Keys.CONTROL, \"v\")\n",
    "        elem_pw.send_keys(\"\\n\")\n",
    "#         browser.find_element(\"xpath\",\"/html/body/div[1]/div[1]/div[1]/div/div/div/div[2]/div/div[1]/form/div[2]/button\").click()\n",
    "        time.sleep(3)\n",
    "        \n",
    "        #search\n",
    "        elem_search = browser.find_element(\"xpath\", \"/html/body/div[1]/div/div[1]/div/div[2]/div[3]/div/div/div/div/div/label/input\")\n",
    "        time.sleep(2)\n",
    "        pyperclip.copy(KEYWORD)\n",
    "        elem_search.send_keys(Keys.CONTROL, \"v\")\n",
    "        elem_search.send_keys(\"\\n\")\n",
    "    #     elem.send_keys(Keys.RETURN)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        #people\n",
    "        elem_people = browser.find_element(\"xpath\", \"/html/body/div[1]/div/div[1]/div/div[3]/div/div/div/div[2]/div[1]/div[1]/div/div[2]/div[1]/div[2]/div/div/div[2]/div/div[3]/div/a/div[1]\")\n",
    "        elem_people.click()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        #country\n",
    "        \n",
    "        # scroll to bottom\n",
    "        last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            time.sleep(1.5)\n",
    "\n",
    "            new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "            \n",
    "        #scrap the profiles' link\n",
    "        soup_profile = bs(browser.page_source, \"html.parser\")\n",
    "        top_elements = soup_profile.find_all(\"a\")\n",
    "\n",
    "        \n",
    "    except:\n",
    "        browser.save_screenshot(\"screenshot_.png\")\n",
    "    #     browser.close()\n",
    "    \n",
    "    \n",
    "    #scroll to bottom\n",
    "#     last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "#     while True:\n",
    "#         print(1)\n",
    "#         browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#         time.sleep(1)\n",
    "#         new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "#         if new_height == last_height:\n",
    "#             print(2)\n",
    "#             break\n",
    "#         last_height = new_height\n",
    "#     print(3)\n",
    "        \n",
    "#     #scrap the profiles' link\n",
    "#     soup_profile = bs(browser.page_source, \"html.parser\")\n",
    "#     top_elements = soup_profile.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3700b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import time\n",
    "import pyperclip\n",
    "\n",
    "def selenium_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--proxy-server=socks5://127.0.0.1:9150\")\n",
    "    options.add_argument('window-size=1920x1080')\n",
    "    options.add_argument('disable-gpu')\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('no-sandox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--start-maximized')\n",
    "    options.add_argument('incognito')\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    browser = webdriver.Chrome(service=service, options=options)\n",
    "    return browser\n",
    "                               \n",
    "if __name__ == \"__main__\":\n",
    "    browser = selenium_driver()\n",
    "    try:\n",
    "        #open browser\n",
    "        browser.get(\"https://www.facebook.com/\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        #log info, search keyword\n",
    "        USER = \"shm8485@gmail.com\"\n",
    "        PWD = \"sohee8485\"\n",
    "        KEYWORD = \"여행\\n\"\n",
    "        \n",
    "        #login\n",
    "        elem_id = browser.find_element(\"id\", \"email\")\n",
    "        pyperclip.copy(USER)\n",
    "        elem_id.send_keys(Keys.CONTROL, \"v\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        elem_pw = browser.find_element(\"id\", \"pass\")\n",
    "        pyperclip.copy(PWD)\n",
    "        elem_pw.send_keys(Keys.CONTROL, \"v\")\n",
    "        elem_pw.send_keys(\"\\n\")\n",
    "#         browser.find_element(\"xpath\",\"/html/body/div[1]/div[1]/div[1]/div/div/div/div[2]/div/div[1]/form/div[2]/button\").click()\n",
    "        time.sleep(3)\n",
    "    \n",
    "        try:\n",
    "            elem_block = browser.find_element(\"xpath\", \"/html/body/div[5]/div[1]/div/div[2]/div/div/div/div/div/div/div[3]/div/div/div/div/div/div/div\")\n",
    "            elem_block.click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #search\n",
    "        browser.get(f\"https://www.facebook.com/search/people/?q={KEYWORD}\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        try:\n",
    "            elem_block = browser.find_element(\"xpath\", \"/html/body/div[5]/div[1]/div/div[2]/div/div/div/div/div/div/div[3]/div/div/div/div/div/div/div\")\n",
    "            elem_block.click()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        \n",
    "        # scroll to bottom\n",
    "        last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            time.sleep(1.5)\n",
    "\n",
    "            new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "            \n",
    "        #scrap the profiles' link\n",
    "        soup_profile = bs(browser.page_source, \"html.parser\")\n",
    "        top_elements = soup_profile.find_all(\"a\")\n",
    "\n",
    "        \n",
    "    except:\n",
    "        browser.save_screenshot(\"screenshot_.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "39566084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scroll to bottom\n",
    "last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e267e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #scrap the profiles' link\n",
    "    soup_profile = bs(browser.page_source, \"html.parser\")\n",
    "    top_elements = soup_profile.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7502c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_href = []\n",
    "\n",
    "for i in top_elements:\n",
    "    href = i.attrs['href']\n",
    "    final_href.append(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ab9e4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.facebook.com/profile.php?id=\"\n",
    "lists = []\n",
    "\n",
    "for i in final_href:\n",
    "    if url in i:\n",
    "        a_lists = i.split(\"&\")\n",
    "        lists.append(a_lists)\n",
    "final_list = [i for i in lists if len(i) <= 1]\n",
    "print(len(final_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "392eb04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name, contact, country = [], [], []\n",
    "final_list = final_list[1:]\n",
    "for i in final_list:\n",
    "    try:\n",
    "        browser.get(i[0]+\"&sk=about_contact_and_basic_info\")\n",
    "        time.sleep(1)\n",
    "        soup_info = bs(browser.page_source, \"html.parser\")\n",
    "        soup_name = soup_info.find(class_=\"x78zum5 x15sbx0n x5oxk1f x1jxijyj xym1h4x xuy2c7u x1ltux0g xc9uqle\")\n",
    "        name.append(soup_name.find(\"h1\").text)\n",
    "        soup_info_contact = soup_info.find(class_=\"xyamay9 xqmdsaz x1gan7if x1swvt13\")\n",
    "        contact.append(soup_info_contact.text)\n",
    "        browser.get(i[0]+\"&sk=about_places\")\n",
    "        time.sleep(1)\n",
    "        soup_info = bs(browser.page_source, \"html.parser\")\n",
    "        soup_info_ctr = soup_info.find(class_=\"xyamay9 xqmdsaz x1gan7if x1swvt13\")\n",
    "        country.append(soup_info_ctr.text)\n",
    "    except AttributeError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e98f891f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['연락처 정보+82 10-8262-6228휴대폰웹사이트 및 소셜 링크표시할 링크 없음기본 정보남성성별', '연락처 정보+82 10-9612-2352휴대폰웹사이트 및 소셜 링크표시할 링크 없음기본 정보남성성별', '연락처 정보+82 10-3654-1504휴대폰sojungland@naver.com이메일웹사이트 및 소셜 링크https://www.jejubaddam.com/웹사이트https://blog.naver.com/sojungland웹사이트https://youtu.be/-RMnC7LUkYU웹사이트기본 정보여성성별1월 5일생일(연도 제외)']\n"
     ]
    }
   ],
   "source": [
    "test = [i for i in contact if '+82' in i]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32d9b4ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m이름\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m연락처\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mcontact\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m지역\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_pd.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    658\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    659\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    666\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:666\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    664\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 666\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    670\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    671\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"이름\":name,\n",
    "                   \"연락처\":contact,\n",
    "                   \"지역\":country})\n",
    "df.to_csv(\"output_pd.csv\", encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ff5d449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "54\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "print(len(name))\n",
    "print(len(contact))\n",
    "print(len(country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d579aff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(final_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fb3a63",
   "metadata": {},
   "source": [
    "### 예전 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d0d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.random\n",
    "\n",
    "name, contact, country = [], [], []\n",
    "final_list = final_list[1:5]\n",
    "for i in final_list:\n",
    "    browser.get(i[0]+\"&sk=about_contact_and_basic_info\")\n",
    "    time.sleep(1)\n",
    "    soup2 = bs(browser.page_source, \"html.parser\")\n",
    "    soup_name = soup2\n",
    "    soup2 = soup2.find(class_=\"xyamay9 xqmdsaz x1gan7if x1swvt13\")\n",
    "    name.append(soup2.find(\"h1\").text)\n",
    "    contact.append(soup2.text)\n",
    "    browser.get(i[0]+\"&sk=about_places\")\n",
    "    time.sleep(1)\n",
    "    soup2 = bs(browser.page_source, \"html.parser\")\n",
    "    soup2 = soup2.find(class_=\"xyamay9 xqmdsaz x1gan7if x1swvt13\")\n",
    "    country.append(soup2.text)\n",
    "    ua.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b4a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get(final_list[1][0]+\"&sk=about_places\")\n",
    "soup2 = bs(browser.page_source, \"html.parser\")\n",
    "print(final_list[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85df28fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get(final_list[2][0]+\"&sk=about_places\")\n",
    "soup2 = bs(browser.page_source, \"html.parser\")\n",
    "print(soup2.find(\"h1\").text)\n",
    "soup2 = soup2.find(class_=\"xyamay9 xqmdsaz x1gan7if x1swvt13\")\n",
    "information = soup2.text\n",
    "print(information)\n",
    "\n",
    "browser.get(final_list[2][0]+\"&sk=about_contact_and_basic_info\")\n",
    "soup2 = bs(browser.page_source, \"html.parser\")\n",
    "soup2 = soup2.find(class_=\"xyamay9 xqmdsaz x1gan7if x1swvt13\")\n",
    "information = soup2.text\n",
    "print(information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a28b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get(final_list[1][0]+\"&sk=about_places\")\n",
    "soup2 = bs(browser.page_source, \"html.parser\")\n",
    "print(soup2.find(\"h1\").text)\n",
    "soup2 = soup2.find(class_=\"xyamay9 xqmdsaz x1gan7if x1swvt13\")\n",
    "information = soup2.text\n",
    "print(information)\n",
    "\n",
    "browser.get(final_list[1][0]+\"&sk=about_contact_and_basic_info\")\n",
    "soup2 = bs(browser.page_source, \"html.parser\")\n",
    "soup2 = soup2.find(class_=\"xyamay9 xqmdsaz x1gan7if x1swvt13\")\n",
    "information = soup2.text\n",
    "print(information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a4f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent(browsers=['chrome'])\n",
    "ua.random\n",
    "\n",
    "for i in final_list:\n",
    "    browser.get(i[0]+\"&sk=about_places\")\n",
    "    time.sleep(0.5)\n",
    "    soup2 = bs(browser.page_source, \"html.parser\")\n",
    "    soup2 = soup2.find(class_=\"xyamay9 xqmdsaz x1gan7if x1swvt13\")\n",
    "    information = soup2.text\n",
    "    print(information)\n",
    "    ua.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d7129",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get(final_list[0][0]+\"&sk=about_places\")\n",
    "soup2 = bs(browser.page_source, \"html.parser\")\n",
    "print(soup2.find(\"h1\").text)\n",
    "soup2 = soup2.find(class_=\"xyamay9 xqmdsaz x1gan7if x1swvt13\")\n",
    "information = soup2.text\n",
    "print(information)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
